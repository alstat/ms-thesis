{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple \"(nabla_b, nabla_w)\" representing the\n",
    "        gradient for the cost function C_x.  \"nabla_b\" and\n",
    "        \"nabla_w\" are layer-by-layer lists of numpy arrays, similar\n",
    "        to \"self.biases\" and \"self.weights\".\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "def cost_derivative(output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Network([3, 4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 1]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.16206226],\n",
       "        [-0.70030588],\n",
       "        [-0.62239334],\n",
       "        [ 1.66877834]]), array([[ 0.32870336]])]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.30313304, -0.51086022,  0.73072451],\n",
       "        [-0.60354507,  0.19370659,  1.06688532],\n",
       "        [ 2.19522426,  0.38280142,  1.113829  ],\n",
       "        [ 2.21620655, -0.11110447,  0.88707761]]),\n",
       " array([[ 0.34983661,  0.42500422, -1.67303179, -0.18466192]])]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = zip(net.biases, net.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.array([np.random.randn(3)]).reshape((3, 1))# for i in np.arange(10)])#.reshape((3, 10))\n",
    "y = np.random.randn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nabla_b = [np.zeros(b.shape) for b in net.biases]\n",
    "nabla_w = [np.zeros(w.shape) for w in net.weights]\n",
    "\n",
    "activation = x\n",
    "activations = [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = net.biases[0]\n",
    "w = net.weights[0]\n",
    "\n",
    "z = np.dot(w, activation) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zs = [] # list to store all the z vectors, layer by layer\n",
    "for b, w in zip(net.biases, net.weights):\n",
    "    z = np.dot(w, activation)+b\n",
    "    zs.append(z)\n",
    "    activation = sigmoid(z)\n",
    "    activations.append(activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delta = cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "nabla_b[-1] = delta\n",
    "nabla_w[-1] = np.dot(delta, activations[-2].transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01646985]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.],\n",
       "        [ 0.]]), array([[ 0.01646985]])]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nabla_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]]),\n",
       " array([[ 0.00397544,  0.00346666,  0.00889029,  0.01482857]])]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nabla_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.14514487],\n",
       "       [-1.32200454],\n",
       "       [ 0.15950602],\n",
       "       [ 2.20108425]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = 2\n",
    "z = zs[-l]\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00105506],\n",
       "       [ 0.00116323],\n",
       "       [-0.00684501],\n",
       "       [-0.00027288]])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = sigmoid_prime(z)\n",
    "delta = np.dot(net.weights[-l+1].transpose(), delta) * sp\n",
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nabla_b[-l] = delta\n",
    "nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.00105506],\n",
       "        [ 0.00116323],\n",
       "        [-0.00684501],\n",
       "        [-0.00027288]]), array([[ 0.01646985]])]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nabla_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.00048485,  0.00077467, -0.00048118],\n",
       "        [ 0.00053456,  0.0008541 , -0.00053051],\n",
       "        [-0.0031456 , -0.00502594,  0.00312178],\n",
       "        [-0.0001254 , -0.00020036,  0.00012445]]),\n",
       " array([[ 0.00397544,  0.00346666,  0.00889029,  0.01482857]])]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nabla_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4595466 ,  0.73424797, -0.45606611]])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations[-l-1].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00048485,  0.00077467, -0.00048118],\n",
       "       [ 0.00053456,  0.0008541 , -0.00053051],\n",
       "       [-0.0031456 , -0.00502594,  0.00312178],\n",
       "       [-0.0001254 , -0.00020036,  0.00012445]])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(delta, activations[-l-1].transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.00105506],\n",
       "        [ 0.00116323],\n",
       "        [-0.00684501],\n",
       "        [-0.00027288]]), array([[ 0.01646985]])]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nabla_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.00048485,  0.00077467, -0.00048118],\n",
       "        [ 0.00053456,  0.0008541 , -0.00053051],\n",
       "        [-0.0031456 , -0.00502594,  0.00312178],\n",
       "        [-0.0001254 , -0.00020036,  0.00012445]]),\n",
       " array([[ 0.00397544,  0.00346666,  0.00889029,  0.01482857]])]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nabla_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4,1) and (4,1) not aligned: 1 (dim 1) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-226-93b707d8bd48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid_prime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mnabla_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnabla_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (4,1) and (4,1) not aligned: 1 (dim 1) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "for l in np.arange(2, net.num_layers):\n",
    "    z = zs[-l]\n",
    "    sp = sigmoid_prime(z)\n",
    "    delta = np.dot(net.weights[-l+1].transpose(), delta) * sp\n",
    "    nabla_b[-l] = delta\n",
    "    nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(2, net.num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[ 0.],\n",
       "         [ 0.],\n",
       "         [ 0.],\n",
       "         [ 0.]]), array([[-0.07345446]])], [array([[ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]]),\n",
       "  array([[-0.03003113, -0.02762548, -0.02711768, -0.07210495]])])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-2d4afc537055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\"nabla_w\"\u001b[0m \u001b[0mare\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0mlists\u001b[0m \u001b[0mof\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m to \"self.biases\" and \"self.weights\".\"\"\"\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnabla_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mnabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# feedforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "        \"\"\"Return a tuple \"(nabla_b, nabla_w)\" representing the\n",
    "        gradient for the cost function C_x.  \"nabla_b\" and\n",
    "        \"nabla_w\" are layer-by-layer lists of numpy arrays, similar\n",
    "        to \"self.biases\" and \"self.weights\".\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10f1c3fd0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqjvpPIsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f0ffe80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "# Make an example plot with two subplots...\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(2,1,1)\n",
    "ax1.plot(range(10), 'b-')\n",
    "\n",
    "ax2 = fig.add_subplot(2,1,2)\n",
    "ax2.plot(range(20), 'r^')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
